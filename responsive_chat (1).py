# -*- coding: utf-8 -*-
"""responsive_chat.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kNSh_YN64aONzHsQ-M06J2aQouMFgscI
"""

#!pip install transformers torch
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from huggingface_hub import login
import random
import re
import time

login(token="insert token here")

"""## Responsive Chat Model


---


#### The following script uses the meta llama-3.2-3B-Instruct text generation model to create a message based on a topic and optional previous message. The previous message allows the model to create a more personalized message(i.e. responding to a message) however it can also create a new message just based off the topic.


---


#### Below are key details about the functions currently used, including their compute time and recommendations for applications.

"""

model_name = "meta-llama/llama-3.2-3B-Instruct"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map="auto")

"""### Pipeline Details
#### **Functions**


---


**1. create_prompt() --> Prompt for the text generation model**

Input Params
* string: title
* list: username_list
* string(optional): previous_message

Output
* string: base_prompt

---
**2. is_valid_message() --> Checks if output of message is correct**

Input Params
* string: message

Output
* boolean: True/False
---
**3. generate_message() --> Runs text generation model and returns the message**

Input Params
* string: title
* list: username_list
* string(optional): previous_message

Output
* string: message


---
Time Cost:
* CPU: ~200s
* GPU - T4: ~1s
---

"""

def create_prompt(title, username_list, previous_message):
    # Prepare the previous message section separately
    if previous_message:
        previous_section = f"The previous message from another person was:\n{previous_message}\nRespond naturally to this message, either by replying directly, adding context, or continuing the discussion."
    else:
        previous_section = "Start a new message about the topic."

    # inserts previous message into main prompt
    base_prompt = f"""
You are generating a single chat message in the subject of **{title}**.
Choose the username **ONLY** from this list: {username_list}.

Message Guidelines (choose one approach at random):
- Share an interesting fact or statistic about the topic.
- Ask a question or express skepticism about the topic.
- Offer diplomatic or balanced thoughts on the topic.
- Be provocative, sarcastic, or playful about the topic.
- Share a personal experience or opinion related to the topic.

{previous_section}

**IMPORTANT INSTRUCTIONS:**
- Output MUST be in the exact format:
  Username: Message
- Do NOT include quotes, brackets, bullet points, or any extra formatting.
- Do NOT include explanations, prefaces, or any additional text.
- Use natural language that sounds like a real person chatting about **{title}**.

**EXAMPLE OUTPUT (format only, content will change):**
Player1: Honestly, I still can't get past level 3 â€” this game is brutal.

Now, generate exactly ONE message.
Output ONLY the message in the format: Username: Message
"""
    return base_prompt

def is_valid_message(message):
    """Check if the message follows the correct format"""
    # Basic format check
    if not ': ' in message:
        return False

    # Split into username and message
    parts = message.split(': ', 1)
    if len(parts) != 2:
        return False

    username, content = parts

    # Username checks
    if len(username) < 3 or len(username) > 30:  # reasonable username length
        return False
    if 'user' in username.lower():  # avoid generic usernames
        return False

    # Message content checks
    if len(content) < 10 or len(content) > 200:  # reasonable message length
        return False

    return True

def generate_message(title, username_list, previous_message):
    start_time = time.time()
    try:
        inputs = tokenizer(create_prompt(title, username_list, previous_message), return_tensors="pt", truncation=True).to(model.device)

        outputs = model.generate(
            **inputs,
            max_new_tokens=50,
            do_sample=True,
            temperature=0.95,
            top_p=0.95,
            no_repeat_ngram_size=3,
            pad_token_id=tokenizer.eos_token_id
        )

        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        messages = [line.strip() for line in generated_text.split('\n') if ': ' in line]

        if messages:
            message = messages[-1]
            if is_valid_message(message):
                elapsed = time.time() - start_time
                print(f"time taken: {elapsed}")
                return message

    except Exception as e:
        print("Failed:", e)
    elapsed = time.time() - start_time
    print(f"time taken: {elapsed}")
    return None

result = generate_message("Mario Run", ["player1", "player2", "player3"], "hey guys how do you like the game")

result

